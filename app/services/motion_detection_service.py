"""
Motion Detection Service - wykrywanie ruchu w nagraniach wideo.

Serwis do detekcji segmentÃ³w z ruchem w nagraniach spawalniczych.
UÅ¼ywa cv2.absdiff do porÃ³wnywania kolejnych klatek.
"""

import cv2  # type: ignore
import numpy as np
import logging
from pathlib import Path
from typing import Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class MotionSegment:
    """Segment wideo z ruchem."""
    start_frame: int
    end_frame: int
    start_time_ms: float
    end_time_ms: float
    duration_ms: float


@dataclass
class MotionAnalysisResult:
    """Wynik analizy ruchu w wideo."""
    filename: str
    total_frames: int
    fps: float
    duration_seconds: float
    segments: list[MotionSegment]
    motion_percentage: float  # Procent klatek z ruchem


class MotionDetectionService:
    """
    Serwis do detekcji ruchu w nagraniach wideo.
    
    UÅ¼ycie:
        service = MotionDetectionService()
        result = service.detect_motion("recordings/rec_20260105_120000.mp4")
        
        # Przytnij wideo do segmentÃ³w z ruchem
        service.trim_to_motion("input.mp4", "output.mp4")
    """
    
    def __init__(
        self,
        recordings_dir: Path = Path("recordings"),
        threshold: int = 25,           # PrÃ³g rÃ³Å¼nicy pikseli
        min_area_percent: float = 0.5, # Min % powierzchni ze zmianÄ…
        min_segment_frames: int = 5,   # Min klatek na segment
        padding_frames: int = 30       # Padding przed/po segmencie (0.5s @60fps)
    ):
        self.recordings_dir = recordings_dir
        self.threshold = threshold
        self.min_area_percent = min_area_percent
        self.min_segment_frames = min_segment_frames
        self.padding_frames = padding_frames
        logger.info("ðŸ” MotionDetectionService initialized")
    
    def detect_motion(
        self,
        video_path: str | Path,
        threshold: Optional[int] = None,
        min_area_percent: Optional[float] = None,
        analyze_step: int = 1
    ) -> MotionAnalysisResult:
        """
        Analizuje wideo i wykrywa segmenty z ruchem.
        
        Args:
            video_path: ÅšcieÅ¼ka do pliku wideo
            threshold: PrÃ³g rÃ³Å¼nicy pikseli (0-255)
            min_area_percent: Minimalny % powierzchni ze zmianÄ…
            analyze_step: Co ktÃ³ra klatka analizowaÄ‡ (1 = kaÅ¼da)
            
        Returns:
            MotionAnalysisResult z listÄ… segmentÃ³w
        """
        path = self._resolve_path(video_path)
        threshold = threshold or self.threshold
        min_area = min_area_percent or self.min_area_percent
        
        cap = cv2.VideoCapture(str(path))
        if not cap.isOpened():
            raise ValueError(f"Cannot open video: {path}")
        
        try:
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
            duration = total_frames / fps if fps > 0 else 0
            
            logger.info(f"ðŸŽ¬ Analyzing motion in {path.name} ({total_frames} frames)")
            
            # Wczytaj pierwszÄ… klatkÄ™
            ret, prev_frame = cap.read()
            if not ret:
                raise ValueError("Cannot read first frame")
            
            prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
            prev_gray = cv2.GaussianBlur(prev_gray, (21, 21), 0)
            
            frame_height, frame_width = prev_frame.shape[:2]
            total_pixels = frame_width * frame_height
            min_changed_pixels = int(total_pixels * min_area / 100)
            
            motion_frames: list[int] = []
            frame_idx = 1
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                if frame_idx % analyze_step == 0:
                    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                    gray = cv2.GaussianBlur(gray, (21, 21), 0)
                    
                    # RÃ³Å¼nica miÄ™dzy klatkami
                    diff = cv2.absdiff(prev_gray, gray)
                    _, thresh = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)
                    
                    # Policz piksele ze zmianÄ…
                    changed_pixels = cv2.countNonZero(thresh)
                    
                    if changed_pixels >= min_changed_pixels:
                        motion_frames.append(frame_idx)
                    
                    prev_gray = gray
                
                frame_idx += 1
            
            # Grupuj klatki w segmenty
            segments = self._group_into_segments(motion_frames, total_frames, fps)
            
            motion_pct = (len(motion_frames) / (total_frames / analyze_step) * 100) if total_frames > 0 else 0
            
            logger.info(f"âœ… Found {len(segments)} motion segments ({motion_pct:.1f}% motion)")
            
            return MotionAnalysisResult(
                filename=path.name,
                total_frames=total_frames,
                fps=fps,
                duration_seconds=duration,
                segments=segments,
                motion_percentage=round(motion_pct, 2)
            )
        finally:
            cap.release()
    
    def _group_into_segments(
        self, 
        motion_frames: list[int], 
        total_frames: int,
        fps: float
    ) -> list[MotionSegment]:
        """Grupuje klatki z ruchem w ciÄ…gÅ‚e segmenty."""
        if not motion_frames:
            return []
        
        segments = []
        start = motion_frames[0]
        end = motion_frames[0]
        
        # Max przerwa miÄ™dzy klatkami w jednym segmencie (0.5s)
        max_gap = int(fps * 0.5)
        
        for frame in motion_frames[1:]:
            if frame - end <= max_gap:
                end = frame
            else:
                # Dodaj padding i zapisz segment
                seg_start = max(0, start - self.padding_frames)
                seg_end = min(total_frames - 1, end + self.padding_frames)
                
                if seg_end - seg_start >= self.min_segment_frames:
                    segments.append(MotionSegment(
                        start_frame=seg_start,
                        end_frame=seg_end,
                        start_time_ms=seg_start / fps * 1000,
                        end_time_ms=seg_end / fps * 1000,
                        duration_ms=(seg_end - seg_start) / fps * 1000
                    ))
                
                start = frame
                end = frame
        
        # Ostatni segment
        seg_start = max(0, start - self.padding_frames)
        seg_end = min(total_frames - 1, end + self.padding_frames)
        
        if seg_end - seg_start >= self.min_segment_frames:
            segments.append(MotionSegment(
                start_frame=seg_start,
                end_frame=seg_end,
                start_time_ms=seg_start / fps * 1000,
                end_time_ms=seg_end / fps * 1000,
                duration_ms=(seg_end - seg_start) / fps * 1000
            ))
        
        # Scal nakÅ‚adajÄ…ce siÄ™ segmenty
        return self._merge_overlapping(segments, fps)
    
    def _merge_overlapping(self, segments: list[MotionSegment], fps: float) -> list[MotionSegment]:
        """Scala nakÅ‚adajÄ…ce siÄ™ lub bliskie segmenty."""
        if len(segments) <= 1:
            return segments
        
        merged = [segments[0]]
        
        for seg in segments[1:]:
            last = merged[-1]
            # JeÅ›li segmenty nakÅ‚adajÄ… siÄ™ lub sÄ… blisko
            if seg.start_frame <= last.end_frame + self.padding_frames:
                # Rozszerz ostatni segment
                merged[-1] = MotionSegment(
                    start_frame=last.start_frame,
                    end_frame=max(last.end_frame, seg.end_frame),
                    start_time_ms=last.start_time_ms,
                    end_time_ms=max(last.end_frame, seg.end_frame) / fps * 1000,
                    duration_ms=(max(last.end_frame, seg.end_frame) - last.start_frame) / fps * 1000
                )
            else:
                merged.append(seg)
        
        return merged
    
    def trim_to_motion(
        self,
        video_path: str | Path,
        output_path: Optional[str | Path] = None,
        threshold: Optional[int] = None,
        min_area_percent: Optional[float] = None,
        include_all_segments: bool = True
    ) -> dict:
        """
        Przycina wideo do segmentÃ³w z ruchem.
        
        Args:
            video_path: ÅšcieÅ¼ka do wideo ÅºrÃ³dÅ‚owego
            output_path: ÅšcieÅ¼ka wyjÅ›ciowa (domyÅ›lnie: {input}_trimmed.mp4)
            threshold: PrÃ³g detekcji ruchu
            min_area_percent: Min % powierzchni ze zmianÄ…
            include_all_segments: True = wszystkie segmenty, False = tylko najdÅ‚uÅ¼szy
            
        Returns:
            SÅ‚ownik z informacjami o przyciÄ™tym wideo
        """
        path = self._resolve_path(video_path)
        
        # Wykryj segmenty
        analysis = self.detect_motion(path, threshold, min_area_percent)
        
        if not analysis.segments:
            logger.warning(f"âš ï¸ No motion detected in {path.name}")
            return {
                "status": "no_motion",
                "filename": path.name,
                "message": "No motion segments detected"
            }
        
        # Wybierz segmenty
        if include_all_segments:
            segments = analysis.segments
        else:
            # Tylko najdÅ‚uÅ¼szy segment
            segments = [max(analysis.segments, key=lambda s: s.duration_ms)]
        
        # Ustal Å›cieÅ¼kÄ™ wyjÅ›ciowÄ…
        if output_path:
            out_path = Path(output_path)
        else:
            out_path = path.parent / f"{path.stem}_trimmed.mp4"
        
        out_path.parent.mkdir(parents=True, exist_ok=True)
        
        # OtwÃ³rz ÅºrÃ³dÅ‚o
        cap = cv2.VideoCapture(str(path))
        if not cap.isOpened():
            raise ValueError(f"Cannot open video: {path}")
        
        try:
            fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            fourcc = cv2.VideoWriter_fourcc(*'mp4v') # type: ignore
            writer = cv2.VideoWriter(str(out_path), fourcc, fps, (width, height))
            
            if not writer.isOpened():
                raise ValueError("Cannot create output video")
            
            frames_written = 0
            
            for seg in segments:
                cap.set(cv2.CAP_PROP_POS_FRAMES, seg.start_frame)
                
                for _ in range(seg.end_frame - seg.start_frame + 1):
                    ret, frame = cap.read()
                    if not ret:
                        break
                    writer.write(frame)
                    frames_written += 1
            
            writer.release()
            
            # Informacje o wyniku
            output_size = out_path.stat().st_size / (1024 * 1024)
            original_size = path.stat().st_size / (1024 * 1024)
            
            logger.info(f"âœ‚ï¸ Trimmed {path.name} -> {out_path.name} ({frames_written} frames)")
            
            return {
                "status": "completed",
                "input_filename": path.name,
                "output_filename": out_path.name,
                "output_path": str(out_path),
                "segments_count": len(segments),
                "frames_written": frames_written,
                "duration_seconds": round(frames_written / fps, 2),
                "original_size_mb": round(original_size, 2),
                "output_size_mb": round(output_size, 2),
                "reduction_percent": round((1 - output_size / original_size) * 100, 1) if original_size > 0 else 0
            }
        finally:
            cap.release()
    
    def _resolve_path(self, video_path: str | Path) -> Path:
        """RozwiÄ…zuje Å›cieÅ¼kÄ™ do pliku wideo."""
        path = Path(video_path)
        # JeÅ›li Å›cieÅ¼ka absolutna lub juÅ¼ istnieje - uÅ¼yj jej
        if path.is_absolute() or path.exists():
            return path
        # W przeciwnym razie szukaj w recordings_dir
        return self.recordings_dir / path


# Singleton dla FastAPI dependency injection
_motion_detection_service: Optional[MotionDetectionService] = None


def get_motion_detection_service() -> MotionDetectionService:
    """FastAPI dependency - zwraca singleton MotionDetectionService."""
    global _motion_detection_service
    if _motion_detection_service is None:
        _motion_detection_service = MotionDetectionService()
    return _motion_detection_service
